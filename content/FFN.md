# 结构

FFN 的结构如下：

$$
FFN(x) = W_2 · Activation(W_1 · x + b_1) + b_2
$$

其中中间维度通常比输入维度更大（例如，在原始 [[Transformer]] 中，输入维度是 512，中间维度是 2048）。扩展维度的作用是让模型在更高维空间中对特征进行组合，形成更复杂的表示。

# FFN 与 MLP

FFN（前馈神经网络）和MLP（多层感知机）实际上是同一个概念的不同名称。两者都指代一种由多层线性变换和非线性激活函数组成的神经网络结构。

FFN 强调的是信息流是单向的，从输入层经过隐藏层到输出层，没有循环或反馈，与之形成对比的是 [[RNN]] 。而 MLP 更偏向于一种具体的实现。

# 在 LLM 中

由于 FFN 中间维度的扩展，它在 Transformer 模型中占据了大部分参数（例如在原始论文中，FFN 参数约占每层总参数的 2/3）。这表明 FFN 是模型的核心计算模块之一，直接影响了模型容量。这个现象在 [[MoE]] 中会变得更加明显。

Attention 机制并没有进行过多的现象变换，它更像是让 embedding 向量之间有一个受到上下文影响的重新分配。而 FFN 才是真正 **拓展、挖掘、拟合语义** 的过程。

# 可解释性

在 [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913) 这篇论文中，提出了一个对于 FFN 的解释：它认为 FFN 并不只是简单的“线性变换-非线性变换-线性变换”，而是可以被解释为一个 Key-Value 数据库， input 就是来这个数据库中查询的一个请求，这个数据库最终会返回查询的结果。

需要注意的是，这里说的 KV 和 Attention 机制中的 KV 并不相同，但是原理是类似的。

论文将第一次线性变换和后续的非现象变换，视为通过 key 来计算 input 与 value 的相关系数，即：

$$
C = Activation(K \cdot x)
$$

而第二次线性变换，就是根据相关系数 $C$ 和 $V$ 来计算 input ，即：

$$
Y = CV
$$
